{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods\n",
    "\n",
    "## Computational Machine Learning assignment - Alessandro Tenderini\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I import the necessary modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import sys\n",
    "sys.path.insert(1, '/Users/utente/desktop/BARCELLONA/TERM 1/Computational Machine Learning/CML_materials')\n",
    "from utils.helper_functions import *\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder       \n",
    "from pandas.api.types import is_string_dtype\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from sklearn import preprocessing                    \n",
    "from sklearn.svm import SVR                           \n",
    "from sklearn.model_selection import GridSearchCV     \n",
    "import dateutil.parser\n",
    "import category_encoders as ce                         \n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer             \n",
    "from sklearn.neighbors import KNeighborsRegressor     \n",
    "from sklearn.metrics import r2_score, mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import set_config\n",
    "from category_encoders import LeaveOneOutEncoder\n",
    "from imblearn.pipeline import Pipeline as Pipeline\n",
    "from category_encoders import WOEEncoder\n",
    "from sklearn.experimental import enable_halving_search_cv \n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.ensemble import ExtraTreesRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import mlens\n",
    "from mlens.visualization import corrmat\n",
    "from mlens.ensemble import SuperLearner\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "I import the train and test datataset. I also import an extra dataset which contains useful information about the diagnosis of each combination of Patient identifier (subject_id) and Hospital stay identifier (hadm_id).\n",
    "\n",
    "\n",
    "* I remove 'Diff' as I do not need realistic datetimes.\n",
    "\n",
    "* I remove 'DIAGNOSIS' as I already have the corresponding ICD9_CODE. \n",
    "\n",
    "* I remove 'HOSPITAL_EXPIRE_FLAG' as we are not supposed to know this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/utente/desktop/BARCELLONA/TERM 1/Computational Machine Learning/CML_materials/extended 2/data/mimic_train.csv\") \n",
    "data= data.drop([\"Diff\",\"HOSPITAL_EXPIRE_FLAG\",\"DIAGNOSIS\"], axis=1)  #\"DOD\",\"DISCHTIME\",\"DEATHTIME\",\"Diff\",\"DIAGNOSIS\"\n",
    "\n",
    "X_test = pd.read_csv(\"/Users/utente/desktop/BARCELLONA/TERM 1/Computational Machine Learning/CML_materials/extended 2/data/mimic_test_los.csv\") \n",
    "X_test=X_test.drop([\"Diff\",\"DIAGNOSIS\"], axis=1)\n",
    "\n",
    "y_train = data[\"LOS\"]   \n",
    "\n",
    "#each subject (subject_id) has different diagnosis (ICD9_CODE)\n",
    "ICD9_code= pd.read_csv(\"/Users/utente/desktop/BARCELLONA/TERM 1/Computational Machine Learning/CML_materials/extended 2/data/MIMIC_diagnoses.csv\") \n",
    "ICD9_code.columns=['subject_id', 'hadm_id', 'SEQ_NUM', 'ICD9_CODE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can include more about diagnosis as in ICD9_code there are all the diagnosis of each combination of HADM_ID and SUBJECT_ID. Hence, I can extract a list of diagnosis for each icustay_id in TEST and TRAIN according to the combination of HADM_ID and SUBJECT_ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target encoder and diagnosis dataset \n",
    "\n",
    "* I create a temporary dataset called \"tepm\", which is an INNER JOIN on the combination of 'subject_id' and 'hadm_id' of original data set and the extra diagnosis datatset.\n",
    "\n",
    "* Now, I perform a target encoding on the column containig the different diagnosis as ICD9_CODE. Then, for each diagnosis, I get the portion of people who died among those who had that diagnosis.\n",
    "\n",
    "* I create three new variables for each combination of 'subject_id' and 'hadm_id', based on the previously computed portion of death people. Such variables are the max value between the portions of death people for each diagnosis of a single combination, the mean of these, and the number of diagnosis of each combination.\n",
    "\n",
    "\n",
    "I do the same procedure for the test set with a temporary dataset called \"tepm2\": I transform the dataset based on the target encoder trained on the training set and follow the same steps.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=pd.merge(data, ICD9_code, on=['subject_id','hadm_id'], how='inner')\n",
    "y_train = temp[\"LOS\"]   \n",
    "temp= temp.drop(['LOS'], axis=1)  #\"DOD\",\"DISCHTIME\",\"DEATHTIME\",\"LOS\",\"Diff\",\"DIAGNOSIS\"\n",
    "temp2=pd.merge(X_test, ICD9_code, on=['subject_id','hadm_id'], how='inner')\n",
    "enc = ce.TargetEncoder(cols=['ICD9_CODE'], min_samples_leaf=20, smoothing=10).fit(temp, y_train)\n",
    "temp = enc.transform(temp)\n",
    "temp2 = enc.transform(temp2)\n",
    "\n",
    "data=pd.merge(data, ICD9_code.groupby(['subject_id','hadm_id'],as_index=False)['SEQ_NUM'].count(), on=['subject_id','hadm_id'])\n",
    "data=pd.merge(data, temp.groupby(['subject_id','hadm_id'],as_index=False)['ICD9_CODE'].mean('ICD9_CODE'), on=['subject_id','hadm_id'])\n",
    "data=pd.merge(data, temp.groupby(['subject_id','hadm_id'],as_index=False)['ICD9_CODE'].max('ICD9_CODE'), on=['subject_id','hadm_id'])\n",
    "\n",
    "X_test=pd.merge(X_test, ICD9_code.groupby(['subject_id','hadm_id'],as_index=False)['SEQ_NUM'].count(), on=['subject_id','hadm_id'])\n",
    "X_test=pd.merge(X_test, temp2.groupby(['subject_id','hadm_id'],as_index=False)['ICD9_CODE'].mean('ICD9_CODE'), on=['subject_id','hadm_id'])\n",
    "X_test=pd.merge(X_test, temp2.groupby(['subject_id','hadm_id'],as_index=False)['ICD9_CODE'].max('ICD9_CODE'), on=['subject_id','hadm_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* These are the y_train (lenght of stay) and X_train datatset I start with. I also save the icustay_id of both the train and test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train = data[\"LOS\"]   \n",
    "X_train=data.drop([\"LOS\"], axis=1)\n",
    "\n",
    "icustay_id_test=X_test[\"icustay_id\"]\n",
    "icustay_id_train=X_train[\"icustay_id\"]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age\n",
    "\n",
    "I calculate the age of the patients based on the date of birth (DOB) and the amdission date (ADMITTIME) and I subsequently remove the variable 'DOB' and 'ADMITTIME'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/d2rn46357hl_mm2r9x_598q80000gn/T/ipykernel_12480/2339022325.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['Age'][index]= round((end-start).days/365)\n",
      "/var/folders/y1/d2rn46357hl_mm2r9x_598q80000gn/T/ipykernel_12480/2339022325.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test['Age'][index]= round((end-start).days/365)\n"
     ]
    }
   ],
   "source": [
    "X_train['Age'] = np.nan\n",
    "X_test['Age'] = np.nan\n",
    "\n",
    "for index in range(len(X_train)):\n",
    "    start = dateutil.parser.parse(X_train['DOB'][index])\n",
    "    end = dateutil.parser.parse(X_train['ADMITTIME'][index])\n",
    "    X_train['Age'][index]= round((end-start).days/365)\n",
    "\n",
    "for index in range(len(X_test)):\n",
    "    start = dateutil.parser.parse(X_test['DOB'][index])\n",
    "    end = dateutil.parser.parse(X_test['ADMITTIME'][index])\n",
    "    X_test['Age'][index]= round((end-start).days/365)\n",
    "    \n",
    "    \n",
    "X_test=X_test.drop(['DOB'], axis=1)  \n",
    "X_train=X_train.drop(['DOB'], axis=1) \n",
    "\n",
    "X_train.loc[X_train['Age'] >=90, 'Age'] = 90\n",
    "X_test.loc[X_test['Age'] >=90, 'Age'] = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Ethnicity \n",
    "\n",
    "I combine some levels of ethnicity to reduce the cardinality of the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WHITE                   3849\n",
       "BLACK                    532\n",
       "UNKNOWN                  352\n",
       "HISPANIC OR LATINO       203\n",
       "ASIAN                    144\n",
       "OTHER                    129\n",
       "MULTI RACE ETHNICITY      12\n",
       "Name: ETHNICITY, dtype: int64"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# A) Combing Ethnicity (train set)\n",
    "X_train['ETHNICITY'] = X_train['ETHNICITY'].replace(['ASIAN', 'ASIAN - CHINESE', 'ASIAN - ASIAN INDIAN', 'ASIAN - VIETNAMESE', 'ASIAN - FILIPINO', 'ASIAN - CAMBODIAN',\n",
    "                                                     'ASIAN - JAPANESE', 'ASIAN - THAI', 'ASIAN - OTHER', 'ASIAN - KOREAN'\n",
    "                                                     ], 'ASIAN')\n",
    "\n",
    "X_train['ETHNICITY'] = X_train['ETHNICITY'].replace(['HISPANIC 0R LATINO', 'HISPANIC/LATINO - PUERTO RICAN', 'HISPANIC/LATINO - DOMINICAN',\n",
    "                                                     'HISPANIC/LATINO - GUATEMALAN', 'HISPANIC/LATINO - CUBAN', 'HISPANIC/LATINO - SALVADORAN',\n",
    "                                                     'HISPANIC/LATINO - MEXICAN', 'HISPANIC/LATINO - CENTRAL AMERICAN (OTHER)', 'HISPANIC/LATINO - COLOMBIAN',\n",
    "                                                     'HISPANIC/LATINO - HONDURAN', 'SOUTH AMERICAN'\n",
    "                                                     ], 'HISPANIC OR LATINO')\n",
    "\n",
    "X_train['ETHNICITY'] = X_train['ETHNICITY'].replace(['WHITE', 'WHITE - RUSSIAN', 'WHITE - OTHER EUROPEAN', 'WHITE - EASTERN EUROPEAN',\n",
    "                                                     'WHITE - BRAZILIAN'\n",
    "                                                     ], 'WHITE')\n",
    "\n",
    "X_train['ETHNICITY'] = X_train['ETHNICITY'].replace(['BLACK/AFRICAN', 'BLACK/AFRICAN AMERICAN', 'BLACK/CAPE VERDEAN', 'BLACK/HAITIAN'\n",
    "                                                     ], 'BLACK')\n",
    "\n",
    "X_train['ETHNICITY'] = X_train['ETHNICITY'].replace(['UNABLE TO OBTAIN', 'UNKNOWN/NOT SPECIFIED', 'PATIENT DECLINED TO ANSWER'\n",
    "                                                     ], 'UNKNOWN')\n",
    "\n",
    "X_train['ETHNICITY'] = X_train['ETHNICITY'].replace(['AMERICAN INDIAN/ALASKA NATIVE', 'AMERICAN INDIAN/ALASKA NATIVE FEDERALLY RECOGNIZED TRIBE',\n",
    "                                                     'CARIBBEAN ISLAND', 'MIDDLE EASTERN', 'OTHER', 'PORTUGUESE', 'NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER',\n",
    "                                                     'MULTI RACE ethnicity'\n",
    "                                                      ], 'OTHER')\n",
    "\n",
    "# B) Combing Ethnicity (test set)\n",
    "X_test['ETHNICITY'] = X_test['ETHNICITY'].replace(['ASIAN', 'ASIAN - CHINESE', 'ASIAN - ASIAN INDIAN', 'ASIAN - VIETNAMESE', 'ASIAN - FILIPINO', 'ASIAN - CAMBODIAN',\n",
    "                                                     'ASIAN - JAPANESE', 'ASIAN - THAI', 'ASIAN - OTHER', 'ASIAN - KOREAN'\n",
    "                                                     ], 'ASIAN')\n",
    "\n",
    "X_test['ETHNICITY'] = X_test['ETHNICITY'].replace(['HISPANIC 0R LATINO', 'HISPANIC/LATINO - PUERTO RICAN', 'HISPANIC/LATINO - DOMINICAN',\n",
    "                                                     'HISPANIC/LATINO - GUATEMALAN', 'HISPANIC/LATINO - CUBAN', 'HISPANIC/LATINO - SALVADORAN',\n",
    "                                                     'HISPANIC/LATINO - MEXICAN', 'HISPANIC/LATINO - CENTRAL AMERICAN (OTHER)', 'HISPANIC/LATINO - COLOMBIAN',\n",
    "                                                     'HISPANIC/LATINO - HONDURAN', 'SOUTH AMERICAN'\n",
    "                                                     ], 'HISPANIC OR LATINO')\n",
    "\n",
    "X_test['ETHNICITY'] = X_test['ETHNICITY'].replace(['WHITE', 'WHITE - RUSSIAN', 'WHITE - OTHER EUROPEAN', 'WHITE - EASTERN EUROPEAN',\n",
    "                                                     'WHITE - BRAZILIAN'\n",
    "                                                     ], 'WHITE')\n",
    "\n",
    "X_test['ETHNICITY'] = X_test['ETHNICITY'].replace(['BLACK/AFRICAN', 'BLACK/AFRICAN AMERICAN', 'BLACK/CAPE VERDEAN', 'BLACK/HAITIAN'\n",
    "                                                     ], 'BLACK')\n",
    "\n",
    "X_test['ETHNICITY'] = X_test['ETHNICITY'].replace(['UNABLE TO OBTAIN', 'UNKNOWN/NOT SPECIFIED', 'PATIENT DECLINED TO ANSWER'\n",
    "                                                     ], 'UNKNOWN')\n",
    "\n",
    "X_test['ETHNICITY'] = X_test['ETHNICITY'].replace(['AMERICAN INDIAN/ALASKA NATIVE', 'AMERICAN INDIAN/ALASKA NATIVE FEDERALLY RECOGNIZED TRIBE',\n",
    "                                                     'CARIBBEAN ISLAND', 'MIDDLE EASTERN', 'OTHER', 'PORTUGUESE', 'NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER',\n",
    "                                                     'MULTI RACE ethnicity'\n",
    "                                                      ], 'OTHER')\n",
    "\n",
    "# Count values for features per category\n",
    "X_train['ETHNICITY'].value_counts()\n",
    "X_test['ETHNICITY'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Religion\n",
    "\n",
    "I do the same for religion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CATHOLIC             1898\n",
       "NOT SPECIFIED        1361\n",
       "PROTESTANT QUAKER     697\n",
       "JEWISH                446\n",
       "OTHER                 424\n",
       "UNOBTAINABLE          395\n",
       "Name: RELIGION, dtype: int64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "religion_other = ['HEBREW', 'UNITARIAN-UNIVERSALIST', 'HINDU', 'GREEK ORTHODOX',\"JEHOVAH'S WITNESS\", \"BUDDHIST\", 'MUSLIM', 'OTHER', 'CHRISTIAN SCIENTIST', 'EPISCOPALIAN', 'ROMANIAN EAST. ORTH', '7TH DAY ADVENTIST']\n",
    "X_train['RELIGION'] = X_train['RELIGION'].replace(religion_other, 'OTHER')\n",
    "\n",
    "\n",
    "\n",
    "X_test['RELIGION'] = X_test['RELIGION'].replace(religion_other, 'OTHER')\n",
    "\n",
    "\n",
    "X_train['RELIGION'].value_counts()\n",
    "X_test['RELIGION'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeated visits to ICU\n",
    "\n",
    "I add a variable that defines the number of the visits for a patient (for patients who went in ICU more tha once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"visits_ICU\"] = X_train.sort_values(['subject_id', 'ADMITTIME']).groupby([\"subject_id\"]).cumcount() + 1\n",
    "X_test[\"visits_ICU\"] = X_test.sort_values(['subject_id', 'ADMITTIME']).groupby([\"subject_id\"]).cumcount() + 1\n",
    "\n",
    "\n",
    "X_train[[\"subject_id\", \"ADMITTIME\", \"visits_ICU\"]].sort_values([\"subject_id\", \"ADMITTIME\"]).iloc[:13]\n",
    "\n",
    "# Eliminate ADMITTIME\n",
    "X_train = X_train.drop(['ADMITTIME'], axis = 1)\n",
    "X_test = X_test.drop(['ADMITTIME'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I \"no category\" for Na in MARITAL_STATUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['MARITAL_STATUS'] = X_train['MARITAL_STATUS'].fillna('no category')\n",
    "X_test['MARITAL_STATUS'] = X_test['MARITAL_STATUS'].fillna('no category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I run Foward and Backweard filling with previous visit information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/d2rn46357hl_mm2r9x_598q80000gn/T/ipykernel_12480/748160573.py:2: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  X_train = X_train.groupby(['subject_id'], as_index = False).apply(lambda group: group.ffill())\n",
      "/var/folders/y1/d2rn46357hl_mm2r9x_598q80000gn/T/ipykernel_12480/748160573.py:3: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  X_train = X_train.groupby(['subject_id'], as_index = False).apply(lambda group: group.bfill())\n"
     ]
    }
   ],
   "source": [
    "before=X_train.isnull().sum()\n",
    "X_train = X_train.groupby(['subject_id'], as_index = False).apply(lambda group: group.ffill())\n",
    "X_train = X_train.groupby(['subject_id'], as_index = False).apply(lambda group: group.bfill())\n",
    "after=X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the amount of missing values that were filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject_id          0\n",
       "hadm_id             0\n",
       "icustay_id          0\n",
       "HeartRate_Min     400\n",
       "HeartRate_Max     400\n",
       "HeartRate_Mean    400\n",
       "SysBP_Min         404\n",
       "SysBP_Max         404\n",
       "SysBP_Mean        404\n",
       "DiasBP_Min        404\n",
       "DiasBP_Max        404\n",
       "DiasBP_Mean       404\n",
       "MeanBP_Min        398\n",
       "MeanBP_Max        398\n",
       "MeanBP_Mean       398\n",
       "RespRate_Min      402\n",
       "RespRate_Max      402\n",
       "RespRate_Mean     402\n",
       "TempC_Min         469\n",
       "TempC_Max         469\n",
       "TempC_Mean        469\n",
       "SpO2_Min          403\n",
       "SpO2_Max          403\n",
       "SpO2_Mean         403\n",
       "Glucose_Min        62\n",
       "Glucose_Max        62\n",
       "Glucose_Mean       62\n",
       "GENDER              0\n",
       "ADMISSION_TYPE      0\n",
       "INSURANCE           0\n",
       "RELIGION            0\n",
       "MARITAL_STATUS      0\n",
       "ETHNICITY           0\n",
       "ICD9_diagnosis      0\n",
       "FIRST_CAREUNIT      0\n",
       "SEQ_NUM             0\n",
       "ICD9_CODE_x         0\n",
       "ICD9_CODE_y         0\n",
       "Age                 0\n",
       "visits_ICU          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before-after ##Really nice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do the same for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y1/d2rn46357hl_mm2r9x_598q80000gn/T/ipykernel_12480/645426860.py:1: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  X_test = X_test.groupby(['subject_id'], as_index = False).apply(lambda group: group.ffill())\n",
      "/var/folders/y1/d2rn46357hl_mm2r9x_598q80000gn/T/ipykernel_12480/645426860.py:2: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  X_test = X_test.groupby(['subject_id'], as_index = False).apply(lambda group: group.bfill())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_test = X_test.groupby(['subject_id'], as_index = False).apply(lambda group: group.ffill())\n",
    "X_test = X_test.groupby(['subject_id'], as_index = False).apply(lambda group: group.bfill())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I save the 3 id of the train and test set and drop them from the two dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Id_train=X_train[[\"subject_id\", \"hadm_id\",\"icustay_id\"]]\n",
    "X_train=X_train.drop([\"subject_id\", \"hadm_id\",\"icustay_id\"], axis=1)\n",
    "Id_test=X_test[[\"subject_id\", \"hadm_id\",\"icustay_id\"]]\n",
    "X_test=X_test.drop([\"subject_id\", \"hadm_id\",\"icustay_id\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the situation with missing values. I am going to imput them in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeartRate_Min     1787\n",
       "HeartRate_Max     1787\n",
       "HeartRate_Mean    1787\n",
       "SysBP_Min         1804\n",
       "SysBP_Max         1804\n",
       "SysBP_Mean        1804\n",
       "DiasBP_Min        1805\n",
       "DiasBP_Max        1805\n",
       "DiasBP_Mean       1805\n",
       "MeanBP_Min        1788\n",
       "MeanBP_Max        1788\n",
       "MeanBP_Mean       1788\n",
       "RespRate_Min      1787\n",
       "RespRate_Max      1787\n",
       "RespRate_Mean     1787\n",
       "TempC_Min         2028\n",
       "TempC_Max         2028\n",
       "TempC_Mean        2028\n",
       "SpO2_Min          1800\n",
       "SpO2_Max          1800\n",
       "SpO2_Mean         1800\n",
       "Glucose_Min        191\n",
       "Glucose_Max        191\n",
       "Glucose_Mean       191\n",
       "GENDER               0\n",
       "ADMISSION_TYPE       0\n",
       "INSURANCE            0\n",
       "RELIGION             0\n",
       "MARITAL_STATUS       0\n",
       "ETHNICITY            0\n",
       "ICD9_diagnosis       0\n",
       "FIRST_CAREUNIT       0\n",
       "SEQ_NUM              0\n",
       "ICD9_CODE_x          0\n",
       "ICD9_CODE_y          0\n",
       "Age                  0\n",
       "visits_ICU           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeartRate_Min     507\n",
       "HeartRate_Max     507\n",
       "HeartRate_Mean    507\n",
       "SysBP_Min         513\n",
       "SysBP_Max         513\n",
       "SysBP_Mean        513\n",
       "DiasBP_Min        514\n",
       "DiasBP_Max        514\n",
       "DiasBP_Mean       514\n",
       "MeanBP_Min        509\n",
       "MeanBP_Max        509\n",
       "MeanBP_Mean       509\n",
       "RespRate_Min      508\n",
       "RespRate_Max      508\n",
       "RespRate_Mean     508\n",
       "TempC_Min         592\n",
       "TempC_Max         592\n",
       "TempC_Mean        592\n",
       "SpO2_Min          512\n",
       "SpO2_Max          512\n",
       "SpO2_Mean         512\n",
       "Glucose_Min        52\n",
       "Glucose_Max        52\n",
       "Glucose_Mean       52\n",
       "GENDER              0\n",
       "ADMISSION_TYPE      0\n",
       "INSURANCE           0\n",
       "RELIGION            0\n",
       "MARITAL_STATUS      0\n",
       "ETHNICITY           0\n",
       "ICD9_diagnosis      0\n",
       "FIRST_CAREUNIT      0\n",
       "SEQ_NUM             0\n",
       "ICD9_CODE_x         0\n",
       "ICD9_CODE_y         0\n",
       "Age                 0\n",
       "visits_ICU          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see GENDER ,ADMISSION_TYPE, INSURANCE, RELIGION, MARITAL_STATUS, ETHNICITY, ICD9_diagnosis, FIRST_CAREUNIT, SEQ_NUM, ICD9_CODE_x, ICD9_CODE_y, Age and visits_ICU have no missing values.\n",
    "\n",
    "For the other variables, I am going to imput their missing values in the preprocessing part of the pipeline.\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for the pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I select the numerical and categorical variables. I also eliminate ICD9_diagnosis from categorical variables as, for this variable, I am going to imput NA in slightly different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feat = X_train.select_dtypes(exclude=['object', 'category']).columns\n",
    "cat_feat = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "icd9_feat = ['ICD9_diagnosis']\n",
    "cat_feat = cat_feat.drop(['ICD9_diagnosis'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to impute missing values in numerical variables with KNNImputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_imputer = KNNImputer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to impute missing values in categorical variables with LeaveOneOutEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder = ce.LeaveOneOutEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to impute missing values for ICD9, which is the main diagnosis with TargetEncoder. Also, I tried to do the same with the extra dataset but it was too complicated. Therefore, I target encoded the diagnosis from the extra dataset outside the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/category_encoders/target_encoder.py:122: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/category_encoders/target_encoder.py:127: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n"
     ]
    }
   ],
   "source": [
    "icd9_encoder = ce.TargetEncoder(smoothing = 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, I will use RobustScaler for numerical variables and StandardScaler for categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_scaler = RobustScaler()\n",
    "cat_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the preprocessing pipeline for numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_preprocessing = make_pipeline(\n",
    "    cont_scaler,\n",
    "    cont_imputer\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the one for categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_preprocessing = make_pipeline(\n",
    "    cat_encoder,\n",
    "    cat_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the one only for ICD9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "icd9_preprocessing = make_pipeline(\n",
    "    icd9_encoder,\n",
    "    cat_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I use ColumnTransformer to merge togther the tree pipeline. Indeed, I am able to apply each preprocessing to the corresponding columns.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    [(\"num\", num_preprocessing, num_feat),\n",
    "     (\"cat\", cat_preprocessing, cat_feat),\n",
    "     (\"icd9\", icd9_preprocessing, icd9_feat)\n",
    "     ]\n",
    "    , remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stackining "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose Stacking over bagging and boosting for two reasons. \n",
    "\n",
    "* First, it allows to use heterogeneous weak learners; combining different algorithms could potentially provide better results (also due to the different nature of the algorithms in terms of variance and bias). \n",
    "\n",
    "* Second, stacking combines the weak learners using a meta-model. On the other hand, bagging and boosting combine the base models according deterministic algorithms. Hence, the first model include a further level of analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weak learners \n",
    "\n",
    "These are the base models that I selected: Support vector machines, K-nearest neighbors, Linear regression and Decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR() \n",
    "knn = KNeighborsRegressor()\n",
    "lr = LinearRegression()\n",
    "dt = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating the stacking ensemble model, I have to chose the meta-model to make the final prediction based on combined predictions of the base models.\n",
    "\n",
    "I am going to run two stack models with two different meta-models:\n",
    "\n",
    "* ExtraTreesRegressor\n",
    "\n",
    "* MLPRegressor (multi-layer perceptron model) which is a type of neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ExtraTreesRegressor ensemble model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create the grid of parameters for hyperpameter optimization of the base models and the final estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid1 = {'ensemble__svr__C': [0.1, 1, 10],\n",
    "              'ensemble__svr__gamma': [0.1, 0.25, 0.5, 0.75],\n",
    "              'ensemble__svr__kernel': ['linear', 'rbf', 'poly'],\n",
    "              'ensemble__knn__n_neighbors': [5, 10, 20, 50, 100, 150],\n",
    "              'ensemble__dt__max_depth':[25, 50, 75, 100],\n",
    "              'ensemble__final_estimator__max_depth': [5,25, 50, 75],\n",
    "              'ensemble__final_estimator__min_samples_split': [2, 4, 6],\n",
    "              'ensemble__final_estimator__min_samples_leaf': [1, 2, 4]\n",
    "              }              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create ensemble model with the previously selected weak learners and ExtraTreesRegressor as final estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble1 =StackingRegressor(estimators=[('svr', svr), ('knn', knn), ('lr', lr), ('dt', dt)], final_estimator=ExtraTreesRegressor())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am also creating the pipeline with the preprocessing and then then the actual model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1 = Pipeline([('preprocess', preprocessing),('ensemble', ensemble1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I run the grid search (HalvingGridSearchCV to reduce time) and find the best model according to root mean square error (RMSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search1 = HalvingGridSearchCV(pipeline1, param_grid1, cv=5, n_jobs=-1, verbose=3,scoring ='neg_root_mean_squared_error')\n",
    "#grid_search1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are predictions and RMSE of the best model and the corresponding parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction_ET=grid_search1.predict(X_test)\n",
    "#grid_search1.best_params_\n",
    "#grid_search1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network ensemble model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do the same for Neural Network as final estimator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = MLPRegressor()\n",
    "\n",
    "ensemble2 =StackingRegressor(estimators=[('svr', svr), ('knn', knn), ('lr', lr),  ('dt', dt)], final_estimator=NN)\n",
    "pipeline2 = Pipeline([('preprocess', preprocessing),('ensemble', ensemble2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I change the parameters to allow for hyperpameter optimization for some parameters of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid2 = {'ensemble__svr__C': [0.1, 1, 10],\n",
    "              'ensemble__svr__gamma': [0.1, 0.25, 0.5, 0.75],\n",
    "              'ensemble__svr__kernel': ['linear', 'rbf', 'poly'],\n",
    "              'ensemble__knn__n_neighbors': [5, 10, 20, 50, 100, 150],\n",
    "              'ensemble__dt__max_depth':[25, 50, 75, 100],\n",
    "              'ensemble__final_estimator__activation': ['tanh', 'relu'],\n",
    "              'ensemble__final_estimator__hidden_layer_sizes': [(30,20,10,5), (30,20,10), (20,10,5), (10,5)],\n",
    "              'ensemble__final_estimator__alpha': [0.0001, 0.001, 0.01, 0.05]\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search2 = HalvingGridSearchCV(pipeline2, param_grid2, cv=5, n_jobs=-1, verbose=3,scoring ='neg_root_mean_squared_error')\n",
    "#grid_search2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are predictions and RMSE of the best model and the corresponding parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction_NN=grid_search2.predict(X_test)\n",
    "#grid_search2.best_params_\n",
    "#grid_search2.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model is the following (the pipeline takes really long)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR(C=0.1,gamma=0.25, kernel=\"rbf\") \n",
    "knn = KNeighborsRegressor(n_neighbors=15)\n",
    "dt = DecisionTreeRegressor(max_depth=50)\n",
    "NN = MLPRegressor(activation=\"relu\",hidden_layer_sizes=(20,10,5), alpha=0.01)\n",
    "\n",
    "ensemble_best =StackingRegressor(estimators=[('svr', svr), ('knn', knn), ('lr', lr),  ('dt', dt)], final_estimator=NN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these are the corresponding predictions with the best model, on the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/category_encoders/target_encoder.py:122: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/category_encoders/target_encoder.py:127: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;robustscaler&#x27;,\n",
       "                                                  RobustScaler()),\n",
       "                                                 (&#x27;knnimputer&#x27;, KNNImputer())]),\n",
       "                                 Index([&#x27;HeartRate_Min&#x27;, &#x27;HeartRate_Max&#x27;, &#x27;HeartRate_Mean&#x27;, &#x27;SysBP_Min&#x27;,\n",
       "       &#x27;SysBP_Max&#x27;, &#x27;SysBP_Mean&#x27;, &#x27;DiasBP_Min&#x27;, &#x27;DiasBP_Max&#x27;, &#x27;DiasBP_Mean&#x27;,\n",
       "       &#x27;MeanBP_Min&#x27;, &#x27;MeanBP_Max&#x27;, &#x27;MeanBP_Mean&#x27;, &#x27;RespRate_Min&#x27;,\n",
       "       &#x27;RespRate_Max&#x27;, &#x27;RespRate_Mean&#x27;, &#x27;TempC_Min&#x27;, &#x27;T...\n",
       "      dtype=&#x27;object&#x27;)),\n",
       "                                (&#x27;cat&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;leaveoneoutencoder&#x27;,\n",
       "                                                  LeaveOneOutEncoder()),\n",
       "                                                 (&#x27;standardscaler&#x27;,\n",
       "                                                  StandardScaler())]),\n",
       "                                 Index([&#x27;GENDER&#x27;, &#x27;ADMISSION_TYPE&#x27;, &#x27;INSURANCE&#x27;, &#x27;RELIGION&#x27;, &#x27;MARITAL_STATUS&#x27;,\n",
       "       &#x27;ETHNICITY&#x27;, &#x27;FIRST_CAREUNIT&#x27;],\n",
       "      dtype=&#x27;object&#x27;)),\n",
       "                                (&#x27;icd9&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;targetencoder&#x27;,\n",
       "                                                  TargetEncoder()),\n",
       "                                                 (&#x27;standardscaler&#x27;,\n",
       "                                                  StandardScaler())]),\n",
       "                                 [&#x27;ICD9_diagnosis&#x27;])])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;robustscaler&#x27;,\n",
       "                                                  RobustScaler()),\n",
       "                                                 (&#x27;knnimputer&#x27;, KNNImputer())]),\n",
       "                                 Index([&#x27;HeartRate_Min&#x27;, &#x27;HeartRate_Max&#x27;, &#x27;HeartRate_Mean&#x27;, &#x27;SysBP_Min&#x27;,\n",
       "       &#x27;SysBP_Max&#x27;, &#x27;SysBP_Mean&#x27;, &#x27;DiasBP_Min&#x27;, &#x27;DiasBP_Max&#x27;, &#x27;DiasBP_Mean&#x27;,\n",
       "       &#x27;MeanBP_Min&#x27;, &#x27;MeanBP_Max&#x27;, &#x27;MeanBP_Mean&#x27;, &#x27;RespRate_Min&#x27;,\n",
       "       &#x27;RespRate_Max&#x27;, &#x27;RespRate_Mean&#x27;, &#x27;TempC_Min&#x27;, &#x27;T...\n",
       "      dtype=&#x27;object&#x27;)),\n",
       "                                (&#x27;cat&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;leaveoneoutencoder&#x27;,\n",
       "                                                  LeaveOneOutEncoder()),\n",
       "                                                 (&#x27;standardscaler&#x27;,\n",
       "                                                  StandardScaler())]),\n",
       "                                 Index([&#x27;GENDER&#x27;, &#x27;ADMISSION_TYPE&#x27;, &#x27;INSURANCE&#x27;, &#x27;RELIGION&#x27;, &#x27;MARITAL_STATUS&#x27;,\n",
       "       &#x27;ETHNICITY&#x27;, &#x27;FIRST_CAREUNIT&#x27;],\n",
       "      dtype=&#x27;object&#x27;)),\n",
       "                                (&#x27;icd9&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;targetencoder&#x27;,\n",
       "                                                  TargetEncoder()),\n",
       "                                                 (&#x27;standardscaler&#x27;,\n",
       "                                                  StandardScaler())]),\n",
       "                                 [&#x27;ICD9_diagnosis&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num</label><div class=\"sk-toggleable__content\"><pre>Index([&#x27;HeartRate_Min&#x27;, &#x27;HeartRate_Max&#x27;, &#x27;HeartRate_Mean&#x27;, &#x27;SysBP_Min&#x27;,\n",
       "       &#x27;SysBP_Max&#x27;, &#x27;SysBP_Mean&#x27;, &#x27;DiasBP_Min&#x27;, &#x27;DiasBP_Max&#x27;, &#x27;DiasBP_Mean&#x27;,\n",
       "       &#x27;MeanBP_Min&#x27;, &#x27;MeanBP_Max&#x27;, &#x27;MeanBP_Mean&#x27;, &#x27;RespRate_Min&#x27;,\n",
       "       &#x27;RespRate_Max&#x27;, &#x27;RespRate_Mean&#x27;, &#x27;TempC_Min&#x27;, &#x27;TempC_Max&#x27;, &#x27;TempC_Mean&#x27;,\n",
       "       &#x27;SpO2_Min&#x27;, &#x27;SpO2_Max&#x27;, &#x27;SpO2_Mean&#x27;, &#x27;Glucose_Min&#x27;, &#x27;Glucose_Max&#x27;,\n",
       "       &#x27;Glucose_Mean&#x27;, &#x27;SEQ_NUM&#x27;, &#x27;ICD9_CODE_x&#x27;, &#x27;ICD9_CODE_y&#x27;, &#x27;Age&#x27;,\n",
       "       &#x27;visits_ICU&#x27;],\n",
       "      dtype=&#x27;object&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RobustScaler</label><div class=\"sk-toggleable__content\"><pre>RobustScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNNImputer</label><div class=\"sk-toggleable__content\"><pre>KNNImputer()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat</label><div class=\"sk-toggleable__content\"><pre>Index([&#x27;GENDER&#x27;, &#x27;ADMISSION_TYPE&#x27;, &#x27;INSURANCE&#x27;, &#x27;RELIGION&#x27;, &#x27;MARITAL_STATUS&#x27;,\n",
       "       &#x27;ETHNICITY&#x27;, &#x27;FIRST_CAREUNIT&#x27;],\n",
       "      dtype=&#x27;object&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LeaveOneOutEncoder</label><div class=\"sk-toggleable__content\"><pre>LeaveOneOutEncoder()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">icd9</label><div class=\"sk-toggleable__content\"><pre>[&#x27;ICD9_diagnosis&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TargetEncoder</label><div class=\"sk-toggleable__content\"><pre>TargetEncoder()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ColumnTransformer(transformers=[('num',\n",
       "                                 Pipeline(steps=[('robustscaler',\n",
       "                                                  RobustScaler()),\n",
       "                                                 ('knnimputer', KNNImputer())]),\n",
       "                                 Index(['HeartRate_Min', 'HeartRate_Max', 'HeartRate_Mean', 'SysBP_Min',\n",
       "       'SysBP_Max', 'SysBP_Mean', 'DiasBP_Min', 'DiasBP_Max', 'DiasBP_Mean',\n",
       "       'MeanBP_Min', 'MeanBP_Max', 'MeanBP_Mean', 'RespRate_Min',\n",
       "       'RespRate_Max', 'RespRate_Mean', 'TempC_Min', 'T...\n",
       "      dtype='object')),\n",
       "                                ('cat',\n",
       "                                 Pipeline(steps=[('leaveoneoutencoder',\n",
       "                                                  LeaveOneOutEncoder()),\n",
       "                                                 ('standardscaler',\n",
       "                                                  StandardScaler())]),\n",
       "                                 Index(['GENDER', 'ADMISSION_TYPE', 'INSURANCE', 'RELIGION', 'MARITAL_STATUS',\n",
       "       'ETHNICITY', 'FIRST_CAREUNIT'],\n",
       "      dtype='object')),\n",
       "                                ('icd9',\n",
       "                                 Pipeline(steps=[('targetencoder',\n",
       "                                                  TargetEncoder()),\n",
       "                                                 ('standardscaler',\n",
       "                                                  StandardScaler())]),\n",
       "                                 ['ICD9_diagnosis'])])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_temp = preprocessing.transform(X_train)\n",
    "columns_X = X_train.columns\n",
    "X_train_temp = pd.DataFrame(X_train_temp,columns=columns_X)\n",
    "\n",
    "X_test_temp = preprocessing.transform(X_test)\n",
    "X_test_temp = pd.DataFrame(X_test_temp,columns=columns_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StackingRegressor(estimators=[(&#x27;svr&#x27;, SVR(C=0.1, gamma=0.25)),\n",
       "                              (&#x27;knn&#x27;, KNeighborsRegressor(n_neighbors=15)),\n",
       "                              (&#x27;lr&#x27;, LinearRegression()),\n",
       "                              (&#x27;dt&#x27;, DecisionTreeRegressor(max_depth=50))],\n",
       "                  final_estimator=MLPRegressor(alpha=0.01,\n",
       "                                               hidden_layer_sizes=(20, 10, 5)))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StackingRegressor</label><div class=\"sk-toggleable__content\"><pre>StackingRegressor(estimators=[(&#x27;svr&#x27;, SVR(C=0.1, gamma=0.25)),\n",
       "                              (&#x27;knn&#x27;, KNeighborsRegressor(n_neighbors=15)),\n",
       "                              (&#x27;lr&#x27;, LinearRegression()),\n",
       "                              (&#x27;dt&#x27;, DecisionTreeRegressor(max_depth=50))],\n",
       "                  final_estimator=MLPRegressor(alpha=0.01,\n",
       "                                               hidden_layer_sizes=(20, 10, 5)))</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>svr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVR</label><div class=\"sk-toggleable__content\"><pre>SVR(C=0.1, gamma=0.25)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>knn</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsRegressor</label><div class=\"sk-toggleable__content\"><pre>KNeighborsRegressor(n_neighbors=15)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>dt</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_depth=50)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(alpha=0.01, hidden_layer_sizes=(20, 10, 5))</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "StackingRegressor(estimators=[('svr', SVR(C=0.1, gamma=0.25)),\n",
       "                              ('knn', KNeighborsRegressor(n_neighbors=15)),\n",
       "                              ('lr', LinearRegression()),\n",
       "                              ('dt', DecisionTreeRegressor(max_depth=50))],\n",
       "                  final_estimator=MLPRegressor(alpha=0.01,\n",
       "                                               hidden_layer_sizes=(20, 10, 5)))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_best.fit(X_train_temp,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=ensemble_best.predict(X_test_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_submit_ensemble=pd.DataFrame({\"icustay_id\":icustay_id_test, \"LOS\": y_pred})\n",
    "prediction_submit_ensemble.to_csv(\"prediction_submit_ensemble.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, I found stacking a really interesting ensemble method. I think that this approach can be effective when the weak learners are diverse as it allows to combine their complementary strengths. Indeed, the predictions from the weak learners are combined using a meta algorithm (like a neural network) to make a final prediction.\n",
    "\n",
    "This reasoning stems from the nature of bagging and boosting. Indeed, Bagging usually reduces the variance of the models while Boosting reduces the bias. Therefore, for Bagging algorithm it would makes sense to use low bias high variance weak learners and for Boosting it is better to use low variance high bias base models.\n",
    "\n",
    "Hence, stacking can be an interesting method as we can include different models with different nature in terms of bias-variance tradeoff."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
